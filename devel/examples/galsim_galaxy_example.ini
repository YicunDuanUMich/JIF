[runtime]
sampler = emcee

[emcee]
walkers = 32
samples = 500
; This is the interval at which convergence diagnostics are performed
nsteps = 100

[pymc]
nsteps = 500
samples = 30000
adaptive_mcmc = True
;covmat = covmat.txt
; In adaptive MCMC the proposal covariance
; is gradually tuned whilst maintaining the overall
; chain convergence properties.  You could set:
; adaptive_mcmc = True
; and you would not need a covariance matrix.
; This converges faster than if you don't have a good
; covariance matrix but a little slower than if you have
; a good one.  It is a good choice for an initial run
; to get a good covmat.  In fact, that's how we made
; demos/covmat5.txt

[output]
filename = modules/MBI/JIF/output/roaster.txt
format = text
verbosity= debug

[pipeline]
modules = roaster
values = modules/JIF/examples/galsim_galaxy_values.ini
extra_output =
likelihoods = jif_roaster
quiet=T
debug=F
timing=F

[roaster]
file = modules/MBI/JIF/roaster_interface.py
infiles = modules/MBI/JIF/TestData/test_image_data.h5

[maxlike]
output_ini = modules/JIF/examples/galsim_galaxy_values.ini
output_covmat = modules/JIF/examples/galsim_galaxy.cov
tolerance = 1e-6
; The BFGS method seems to find it a bit harder to actually locate
; the peak, but once it's there it provides you with covariance
; matrix estimate
;method = Nelder-Mead
;method = BFGS
; Any minimizer available in scipy can be specified here - they are:
; Nelder-Mead
; Powell
; CG
; BFGS
; Newton-CG
; L-BFGS-B
; TNC
; COBYLA
; SLSQP
; dogleg
; trust-ncg
